{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00696de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG (adjust paths)\n",
    "DATASET_PATH = \"../../data/En-Ba-Dataset(20k_4)/dataset.csv\"\n",
    "OUTPUT_PATH = \"embeddings.csv\"\n",
    "VOCAB_PATH = \"vocabulary.json\"\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "BITS_PER_NUMBER = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- VOCAB (optional inspection only) ----------------\n",
    "def build_vocab(sentences):\n",
    "    counter = Counter()\n",
    "    for s in sentences:\n",
    "        tokens = s.strip().split()\n",
    "        counter.update(tokens)\n",
    "    items = counter.most_common()\n",
    "    vocab = {word: {\"index\": idx, \"count\": cnt} for idx, (word, cnt) in enumerate(items)}\n",
    "    return vocab\n",
    "\n",
    "def save_vocab(vocab, filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved vocab ({len(vocab)} words) to {filepath}\")\n",
    "    for i, (w, info) in enumerate(vocab.items()):\n",
    "        if i >= 20: break\n",
    "        print(f\"{w}: {info['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e576ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- BERT SETUP ----------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LOAD DATA ----------------\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "sentences = df[\"Sentence\"].astype(str).tolist()\n",
    "labels = df[\"Label\"].tolist()\n",
    "print(\"Loaded\", len(sentences), \"sentences\")\n",
    "\n",
    "# Build vocab (analysis only, doesnâ€™t affect embeddings)\n",
    "vocab = build_vocab(sentences)\n",
    "save_vocab(vocab, VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- EMBEDDING ----------------\n",
    "def embed_batch(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH\n",
    "    )\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "        cls_embs = out.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embs\n",
    "\n",
    "\n",
    "all_embs = []\n",
    "for i in tqdm(range(0, len(sentences), BATCH_SIZE), desc=\"Embedding\"):\n",
    "    batch_texts = [\n",
    "        s.strip() for s in sentences[i : i + BATCH_SIZE]\n",
    "    ]  # only strip whitespace\n",
    "    embs = embed_batch(batch_texts)\n",
    "    all_embs.append(embs)\n",
    "\n",
    "embedded_arr = np.vstack(all_embs)\n",
    "print(\"Embedding shape:\", embedded_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- NORMALIZATION ----------------\n",
    "col_min = embedded_arr.min(axis=0)\n",
    "col_max = embedded_arr.max(axis=0)\n",
    "scale = np.where(col_max - col_min == 0, 1, col_max - col_min)\n",
    "\n",
    "normalized = 1 + (embedded_arr - col_min) * (254.0 / scale)\n",
    "normalized = np.round(normalized).astype(np.int32)\n",
    "\n",
    "print(\"Numeric value range:\", normalized.min(), normalized.max())\n",
    "assert 1 <= normalized.min() and normalized.max() <= 255\n",
    "\n",
    "num_df = pd.DataFrame(normalized)\n",
    "num_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved numeric embeddings to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
